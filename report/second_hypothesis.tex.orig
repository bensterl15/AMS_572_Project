[TO DO: Add a section on missing values research]

The second hypothesis analyzes which environmental factors are the best predicters of temperature for Salt Lake City. According to the United States Environmental Protection Agency (EPA), higher temperature results in either more or less precipation and higher frequency of storms\cite{epa_utah}. This study performs a multiple linear regression of temperature against precipation, number of thunderstorms per month, and minutes of sunlight per month. In conjuction with the EPA article noted above, we also considered two key characteristics when deciding which parameters we would include in our study: 

\begin{itemize}
	\item Linear relationship with temperature
	\item Availability of data
\end{itemize}

For example, we did not include total monthly snowfall or number of days with fog because they have too much missing data despite having a decent linear relationship. The parameter with the most significant linear relationship with temperature is, not suprisingly, minutes of sunlight. A major factor we had to consider here, however, was that sunlight data is only recorded from 1965 to 2004. Thus, in order to include this parameter in our analysis, we needed to restrict our study down to this range (roughly 479 total months, or observations). Additionally, we limited the data used in our study to months for which the data was available for all four parameters listed above. Therefore, when including temperature, precipation, and number of thunderstorms in our study, our sample size decreased from 479 months to 321 months because at least one of the these parameters was missing data in the 479 months of data avialable between 1965 and 2004.

For each of the parameters identified above, we examine its graph by temperature with a line of best fit. As expected, there is a strong positive correlation between minutes of sunlight and temperature, as seen in Figure \ref{fig:temp_vs_sun}. Similarly, albeit not as strong, there is also to be a positive correlation between number of thunderstorms and temperature, as seen in Figure \ref{fig:temp_vs_dthunderstorms}. Lastly, the correlation between precipation and temperature is negative, as seen in Figure \ref{fig:temp_vs_precipation}, which is expected in some regions as stated by the EPA \cite{epa_utah}.

As an additional preliminarly analysis, in order to obtain a strong multiple linear regression model to estimate temperature, we sought to use parameters that are not highly correlated with each other, as including additional variables that are highly correlated is more likely to simply increase model complexity as opposed to imporve the model fit. Figure \ref{fig:correlation_plot}, which is a correlation matrix of the three independent parameters, suggests that the independent variables of sunlight, days of thunderstorm, and precipation are not highly correlated, further supporting our decision to include them as parameters in our model.

<<<<<<< HEAD
At this point, we conducted the analysis on the selected paramters. Per Table \ref{tab:lin_regression}, we can see the the model has an adjusted $R^{2}$ of $0.7969 \approx$ 0.8, meaning that roughly 80.0\% of the variance in temperature can be explained by the three predictors. Using the coefficients from the table, we can estimate temperature as $\hat{T} = \beta_{0} + X_{1}\beta_{1} + X_{2}\beta_{2} + X_{3}\beta_{3}$, where indices 1, 2, and 3 reference minutes of sun, days of thunderstorms, and precipitation, respectively. Substituting $\beta$ values from Table \ref{tab:lin_regression}, $$\hat{T} \approx -5.26 + 1.04 \times 10^{-3}X_{1} + 0.84X_{2} + -4.80\times 10^{-2}X_{3}.$$
=======
At this point, we conducted the analysis on the selected paramters. Per Table \ref{tab:lin_regression}, we can see the the model has an adjusted $R^{2}$ of $\approx$ 80.0\%, meaning that roughly 80.0\% of the variance in temperature can be explained by the three predictors. Using the coefficients from the table, we can estimate temperature as $\hat{T} = \beta_{0} + X_{1}\beta_{1} + X_{2}\beta_{2} + X_{3}\beta_{3}$, where indices 1, 2, and 3 reference minutes of sun, days of thunderstorms, and precipitation, respectively. Substituting $\beta$ values from Table \ref{tab:lin_regression}, $$\hat{T} \approx -5.26 + 1.04 \times 10^{-3}X_{1} + 0.84X_{2} - 4.80\times 10^{-2}X_{3}.$$

[NOTE: FOR THE EQUATION ABOVE, I CHANGED THE + -4.80... TO JUST - 4.80]
>>>>>>> capple

Now that we have completed out multiple linear regression analysis on the three selected parameters, we turn to a final model comparison analysis. Here, we conduct a best subset analysis to locate the best model at each model size, including 1, 2, and 3 parameters. The best subset selection process considers all $3 \choose k$ at each k number of parameters (k = 1, 2, 3). For each k, the best subset method will select the model with the lowest SSE. As can been seen in Figure \ref{tab:optimal_selection}, the best models for k = 1, 2, and 3 are minutes of sun, minutes of sun + days of thunderstorms, and minutes of sun + days of thunderstorms + precipation, respectively.

Now that we have our three best models at each k, we will calculate the following metrics for each model: 

\begin{itemize}
	\item Adjusted $R^{2}$
	\item AIC
	\item BIC
\end{itemize}

The adjusted $R^{2}$ will show what percentage of the variance in the model is explained by the parameters, adjusting downward as model complexity increases. AIC and BIC are both penalized-likelihood criteria, where the BIC penalizes model compexity a bit more than AIC. Overall, the best model of the three will have the greatest adjusted $R^{2}$ and the lowest AIC and BIC values. Define: 

\begin{itemize}
	\item Model 1: Performs regression only minutes of sunlight
	\item Model 2: Performs regression against minutes of sunlight + days of thunderstorm
	\item Model 3: Performs regression against minutes of sunlight + days of thunderstorm + precipitation
\end{itemize} 

Table \ref{tab:model_info} contains the results. the full model, model 3, is the best overall model to choose to estimate temperature because it has the highest $R^{2}$ and lowest AIC and BIC.

To investigate the effects of missing data on the multiple linear regression, we removed 5\% of temperature observations, 5\% of sunlight observations, 5\% of thunderstorm observations, and 5\% of preceipitation observations. We then ignored any data point for which at least one of these four observations was missing, resulting in a smaller sample size. Because of the random nature of the removal of data, the new sample size is different for each run. The new adjusted $R^{2}$ is $0.797 \approx 0.8$, indicating that roughly 80\% of the variation in temperature can be explained by the three predictors. This is not appreciably different from the adjusted $R^{2}$ for the full data set from 1965 to 2004. The new estimate for temperature is $$\hat{T} \approx -4.79 + 1.03 \times 10^{-3}X_{1} + 0.85X_{2} + -6.30\times 10^{-2}X_{3}.$$ See Table \ref{tab:lin_regression_missing_data} for $\beta$ and p-value comparisons for all three predictor variables in the missing data case.

The best models for k = 1, 2, and 3 are still minutes of sun, minutes of sun + days of thunderstorms, and minutes of sun + days of thunderstorms + precipation, respectively; the missing data had no effect on this. The adjusted $R^{2}$, AIC, and BIC are tabulated in Table \ref{tab:model_info_missing_data}, from which we determine that the best model for estimating temperature is the full model, regardless of our new smaller sample size.

Now that we have located the best model, we can interpret our results. Starting with \beta_{1}, for each additional minute of sunshine, the average monthly temperature will increase by $1.04 \times 10^{-3}$ degrees Celsius. Similarly for \beta_{2}, for each additional thunderstorm day, the average monthly temperature will increase by $0.84$ degrees Celsius. Lastly, for each additional millimeter of precipation in a given month, the average monthly temperature will decrease by $4.80\times 10^{-2}$ degrees Celsius.

To demonstrate our results, let us consider the following example to predict the average monthly temperature of a new observation $Y^{*}$. Notationally, the following formula enables the calculation of an $(1-\alpha)$-level prediction interval for $Y^{*}$. $$Y^{*} \pm \hat{Y^{*}} + t_{n-(k+1),\alpha/2}s\sqrt{1+x^{*T}'Vx^{*}},$$ where $\hat{Y^{*}}$ is the estimate of $Y^{*}$, $t_{n-(k+1),\alpha/2}$ is the critical value, s is the estimate for \sigma, equivalent to $\sqrt{MSE}$, $x^{*}$ is the predictor variables for the three parameters, and $V = (X^{T}X)^{-1}, with X representing the matrix of the values of the predictor variables.

[NOTE: NEED THE x^{*}, V, X^{T}X and X TO BE BOLD]

Now, given a new month with 15,000 minutes of sunlight, 4 days with thunderstorms, and 35 millimeters of total precipation, we can calculcate a 95\% prediction interval, as calculated below. As an additional note, to avoid extrapolation, the given parameter figures above are all within the range of values observed for each parameter in the 321 oberservations analyzed. In this example, with 95\% confidence, the average monthly temperature of a new month would be between [INSERT HERE] degrees Celsius and [INSERT HERE] degrees Celsius.

[SEE BELOW FOR INSTRUCTIONS FOR R CODE TO GET PREDICTION INTERVAL]

Add a new value: new_value = data.frame(sun = 15000, thunder = 4, precip = 35)
Calc the PI: predict(fit, new_value, interval = "prediction", level = 0.95)