When data are missing at random, the value of the missing data is not related to the reason for its missingness. In this case, the complete data points are essentially a random sample of the entire dataset, so missing points can be ignored in the analysis. We evaluated the effects of data missing at random on our hypothesis test and multiple linear regression in sections 2 and 3 respectively.

Missing data is non-ignorable if the value of the missing data is related to the reason for its missingness. For example, a storm could cause data collection equipment to become inoperable, and the storm data would not be collected. In this case, it is not possible to ignore the missing data in the analysis because the complete data is no longer a random sample, and therefore does not represent the whole of the data. The Heckman correction is a method that can correct bias from this non-random sample\cite{mnar}. In the context of regression, suppose we want to fit the linear model $$Y_{i} = X_{i}\beta+\epsilon$$ to our data. Heckman's model lets us obtain unbiased estimators of $\beta$. First, we introduce the selection equation $P(R_{yi} = 1 | X_{i}^{s}) = \Phi(X_{i}^{s}\beta^{s})$, where $R_{yi}$ is an indicator variable equal to $1$ if $Y_{i}$ is observed and $0$ if $Y_{i}$ is missing. We use maximum likelihood estimation, a method of estimating parameters by maximizing the likelihood function, to find estimates for $\hat{\beta}^{s}$. Using these estimates, we can compute the inverse Mills ratio $\hat{\lambda}_{i} = \frac{\phi(X_{i}^{s}\hat{\beta}^{s})}{\Phi(X_{i}^{s}\hat{\beta}^{s})}$ for each observation in the sample, where $\phi$ is the standard normal density and $\Phi$ is the standard normal distribution function. Finally, using the linear equation $Y_{i} = X_{i}\beta + \hat{\lambda}_{i}\beta_{\lambda} + \eta_{i}$, where $\eta ~ N(0,\sigma_{\eta}^{2})$, we can find estimators for $\hat{\beta}$ and $\hat{\beta}_{\lambda}$. There also exists a one-step procedure for estimating $\beta$ which generates a lower standard error, but the two-step procedure is used for faster computation.

After obtaining our estimates for $\beta$, we can draw $\sigma_{\eta}^{2*}$, $\beta^{*}$, and $\beta_{\lambda}^{*}$ from the linear equation. Also, we can draw $\eta^{*}$ from $N(0,\sigma_{\eta}^{2*}$. Finally, for each missing $Y$, we can impute $$Y_{i}^{*} = X_{i}\beta^{*} + \frac{-\phi(\widehat{X_{i}^{s}\beta^{s})}}{1-\Phi(\widehat{X_{i}^{s}\beta^{s}})}\beta_{\lambda i}^{*}+\eta^{*}.$$